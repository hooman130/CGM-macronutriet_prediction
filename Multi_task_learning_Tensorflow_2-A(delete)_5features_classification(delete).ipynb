{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_recall(confusion_matrix):\n",
    "    n = 0\n",
    "    accumulated_recall = 0\n",
    "    for row in confusion_matrix:\n",
    "        accumulated_recall += row[n]/np.sum(row)\n",
    "        n += 1\n",
    "        \n",
    "    return accumulated_recall/(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_LBM = {'38A': 39.47,\n",
    "           '38B': 62.78,\n",
    "           '38C': 53.5,\n",
    "           '38D': 37.47,\n",
    "           '38E': 45.52,\n",
    "           '38F': 43.38,\n",
    "           '38H': 47.39}\n",
    "\n",
    "sub_BW = {'38A': 70.1,\n",
    "          '38B': 96.51,\n",
    "          '38C': 84.4,\n",
    "          '38D': 65.1,\n",
    "          '38E': 78.8,\n",
    "          '38F': 68.4,\n",
    "          '38H': 88.8}\n",
    "\n",
    "sub_BMI = {'38A': 30.9,\n",
    "           '38B': 33.7,\n",
    "           '38C': 31,\n",
    "           '38D': 27,\n",
    "           '38E': 32,\n",
    "           '38F': 27.9,\n",
    "           '38H': 29}\n",
    "\n",
    "sub_FM = {'38A':  26.8371,\n",
    "           '38B': 28.7399,\n",
    "           '38C': 24.7426,\n",
    "           '38D': 26.0123,\n",
    "           '38E': 32.4202,\n",
    "           '38F': 23.9422,\n",
    "           '38H': 39.7503}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "#raw data\n",
    "########################################################\n",
    "\n",
    "#spreadsheet = '../../CGM_insulin_TG_data.xlsx'\n",
    "\n",
    "def get_raw_data(blood_analyte, spreadsheet):\n",
    "    #subject_id = '38A'\n",
    "    #blood_analyte = 'CGM'\n",
    "\n",
    "    xl = pd.ExcelFile(spreadsheet)\n",
    "    Meal_dfs = xl.parse(blood_analyte)\n",
    "    sub_meal_idx = {}\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    X=[]\n",
    "    Y1=[]\n",
    "    Y2=[]\n",
    "    Y3=[]\n",
    "\n",
    "    for index, row in Meal_dfs.iterrows():\n",
    "\n",
    "        #ignore missing meals\n",
    "        if not math.isnan(row.tolist()[6]):\n",
    "\n",
    "            #get each subject's meal index\n",
    "            for subject_id in ['38A', '38B', '38C', '38D', '38E', '38F', '38H']:\n",
    "                if row['Patient_ID'] == subject_id:\n",
    "                    if subject_id in sub_meal_idx:\n",
    "                        sub_meal_idx[subject_id] += [index]\n",
    "                    else:\n",
    "                        sub_meal_idx[subject_id] = [index]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        X += [np.float32(one_cgm) for one_cgm in row.tolist()[2:34]]\n",
    "\n",
    "        #carbs\n",
    "        if row['Meal_ID'][4]== '1':\n",
    "            Y1 += [np.float32(52.25)]\n",
    "            #Y1 += [np.float32(1)]\n",
    "        elif row['Meal_ID'][4]== '2':\n",
    "            Y1 += [np.float32(94.75)]\n",
    "            #Y1 += [np.float32(2)]\n",
    "        elif row['Meal_ID'][4]== '3':\n",
    "            Y1 += [np.float32(179.75)]\n",
    "            #Y1 += [np.float32(3)]\n",
    "\n",
    "        #protein\n",
    "        if row['Meal_ID'][1]== '1':\n",
    "            Y2 += [np.float32(15)]\n",
    "            #Y2 += [np.float32(1)]\n",
    "        elif row['Meal_ID'][1]== '2':\n",
    "            Y2 += [np.float32(30)]\n",
    "            #Y2 += [np.float32(2)]\n",
    "        elif row['Meal_ID'][1]== '3':\n",
    "            Y2 += [np.float32(60)]\n",
    "            #Y2 += [np.float32(3)]\n",
    "\n",
    "        #fat\n",
    "        if row['Meal_ID'][7]== '1':\n",
    "            Y3 += [np.float32(13)]    \n",
    "            #Y3 += [np.float32(1)]  \n",
    "        elif row['Meal_ID'][7]== '2':\n",
    "            Y3 += [np.float32(26)]   \n",
    "            #Y3 += [np.float32(2)]  \n",
    "        elif row['Meal_ID'][7]== '3':\n",
    "            Y3 += [np.float32(52)]  \n",
    "            #Y3 += [np.float32(3)]\n",
    "\n",
    "\n",
    "\n",
    "    X_value = np.array(X).reshape(63,32)\n",
    "    scaler = StandardScaler()\n",
    "    #X_value = scaler.fit_transform(X_value)\n",
    "    Y1_value = np.array(Y1).reshape(-1,1)\n",
    "    Y2_value = np.array(Y2).reshape(-1,1)\n",
    "    Y3_value = np.array(Y3).reshape(-1,1)\n",
    "\n",
    "    #one hot encoding\n",
    "    s = pd.Series(Y1_value.reshape(63,))\n",
    "    Y1_value_1hot = pd.get_dummies(s)\n",
    "    Y1_value_1hot = Y1_value_1hot.values\n",
    "\n",
    "    s = pd.Series(Y2_value.reshape(63,))\n",
    "    Y2_value_1hot = pd.get_dummies(s)\n",
    "    Y2_value_1hot = Y2_value_1hot.values\n",
    "\n",
    "    s = pd.Series(Y3_value.reshape(63,))\n",
    "    Y3_value_1hot = pd.get_dummies(s)\n",
    "    Y3_value_1hot = Y3_value_1hot.values\n",
    "\n",
    "\n",
    "    #Y1 = (Y1 - Y1.min())/(Y1.max()-Y1.min())\n",
    "    #Y2 = (Y2 - Y2.min())/(Y2.max()-Y2.min())\n",
    "    #Y3 = (Y3 - Y3.min())/(Y3.max()-Y3.min())\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    return X_value, Y1_value_1hot, Y2_value_1hot, Y3_value_1hot, sub_meal_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# 17 Gaussian AUC features\n",
    "########################################################\n",
    "\n",
    "\n",
    "#data=pd.DataFrame(pd.read_excel('../../CGM_insulin_TG_data.xlsx'))\n",
    "#feature_num can be 5 or 17\n",
    "\n",
    "def get_Gau_features(blood_analyte, spreadsheet, feature_num):\n",
    "\n",
    "    xl = pd.ExcelFile(spreadsheet)\n",
    "    data = xl.parse(blood_analyte)\n",
    "\n",
    "    def process_data(data):\n",
    "        sample_am = data.shape[0]\n",
    "        #deprecated for these labels\n",
    "        protein = data.iloc[:, 1].values.reshape(-1, 1)\n",
    "        cho = data.iloc[:, 2].values.reshape(-1, 1)\n",
    "        fat = data.iloc[:, 3].values.reshape(-1, 1)\n",
    "        # pudding_A = data_38A.iloc[:,4].values.reshape(-1,1)\n",
    "        # water = data_38A.iloc[:,5].values.reshape(-1,1)\n",
    "        total_e = data.iloc[:, 6].values.reshape(-1, 1)\n",
    "        glucose = data.iloc[:, 2:35].values.reshape(sample_am, -1)\n",
    "        #glucose_last_column = glucose[:, -1]\n",
    "        #glucose_r = np.c_[glucose, glucose_last_column]\n",
    "        #return protein, cho, fat, total_e, glucose\n",
    "        return protein, cho, fat, total_e, offset_to_baseline(glucose)\n",
    "\n",
    "    def offset_to_baseline(a):\n",
    "        #a = np.array([[1,2,3],[4,5,6]]) \n",
    "\n",
    "        averages = np.array([])\n",
    "        for idx, row in enumerate(a):\n",
    "            averages = np.append(averages, np.mean(row[:3]))\n",
    "\n",
    "        averages = averages.reshape(a.shape[0], 1)\n",
    "        averages_new = np.tile(averages, a.shape[1])\n",
    "\n",
    "        return a - averages_new\n",
    "\n",
    "    def process_glucose(glucose, feature_num):\n",
    "        a = np.zeros((1, 5))\n",
    "        for row in glucose:\n",
    "            # print(glucose_gaussian_feature_4(row).shape())\n",
    "            # print(glucose_gaussian_feature_8(row).shape())\n",
    "            # print(glucose_gaussian_feature_16(row).shape())\n",
    "            \n",
    "            if feature_num == 17:\n",
    "                glucose_gaussian_feature = np.hstack((glucose_gaussian_feature_4(row),\n",
    "                                                      glucose_gaussian_feature_8(row),\n",
    "                                                      glucose_gaussian_feature_16(row)))\n",
    "            elif feature_num == 5:\n",
    "                glucose_gaussian_feature = np.hstack((glucose_gaussian_feature_8(row)))\n",
    "                \n",
    "            a = np.vstack([a, glucose_gaussian_feature])\n",
    "        # print(a.shape)\n",
    "        a = np.delete(a, 0, axis=0)\n",
    "        # print(a.shape)\n",
    "        return a\n",
    "\n",
    "    def normal_function(mu, sigma, x):\n",
    "        normal_function_value = (1 / (sigma * np.sqrt(2 * np.pi))) * \\\n",
    "            np.exp(-((x - mu) ** 2) / (2 * (sigma ** 2)))\n",
    "        return normal_function_value\n",
    "\n",
    "    def glucose_gaussian_feature_4(glucose_row):\n",
    "        gaussian_feature = []\n",
    "        normal_g_1 = normal_function(\n",
    "            0, 4/1.96, np.asarray([i for i in range(0, 5)])) * glucose_row[0:5]\n",
    "        normal_g_2 = normal_function(\n",
    "            4, 4/1.96, np.asarray([i for i in range(0, 9)])) * glucose_row[0:9]\n",
    "        normal_g_3 = normal_function(\n",
    "            8, 4/1.96, np.asarray([i for i in range(4, 13)])) * glucose_row[4:13]\n",
    "        normal_g_4 = normal_function(\n",
    "            12, 4/1.96, np.asarray([i for i in range(8, 17)])) * glucose_row[8:17]\n",
    "        normal_g_5 = normal_function(\n",
    "            16, 4/1.96, np.asarray([i for i in range(12, 21)])) * glucose_row[12:21]\n",
    "        normal_g_6 = normal_function(\n",
    "            20, 4/1.96, np.asarray([i for i in range(16, 25)])) * glucose_row[16:25]\n",
    "        normal_g_7 = normal_function(\n",
    "            24, 4/1.96, np.asarray([i for i in range(20, 29)])) * glucose_row[20:29]\n",
    "        normal_g_8 = normal_function(\n",
    "            28, 4/1.96, np.asarray([i for i in range(24, 33)])) * glucose_row[24:33]\n",
    "        normal_g_9 = normal_function(\n",
    "            32, 4/1.96, np.asarray([i for i in range(28, 33)])) * glucose_row[28:33]\n",
    "        for i in range(1, 10):\n",
    "            gaussian_feature.append(np.trapz(locals()[\"normal_g_\" + str(i)]))\n",
    "        return gaussian_feature\n",
    "\n",
    "    def glucose_gaussian_feature_8(glucose_row):\n",
    "        gaussian_feature = []\n",
    "        normal_g_1 = normal_function(\n",
    "            0, 8/1.96, np.asarray([i for i in range(0, 9)])) * glucose_row[0:9]\n",
    "        normal_g_2 = normal_function(\n",
    "            8, 8/1.96, np.asarray([i for i in range(0, 17)])) * glucose_row[0:17]\n",
    "        normal_g_3 = normal_function(\n",
    "            16, 8/1.96, np.asarray([i for i in range(8, 25)])) * glucose_row[8:25]\n",
    "        normal_g_4 = normal_function(\n",
    "            24, 8/1.96, np.asarray([i for i in range(16, 33)])) * glucose_row[16:33]\n",
    "        normal_g_5 = normal_function(\n",
    "            32, 8/1.96, np.asarray([i for i in range(24, 33)])) * glucose_row[24:33]\n",
    "        for i in range(1, 6):\n",
    "            gaussian_feature.append(np.trapz(locals()[\"normal_g_\" + str(i)]))\n",
    "        return gaussian_feature\n",
    "\n",
    "    def glucose_gaussian_feature_16(glucose_row):\n",
    "        gaussian_feature = []\n",
    "        normal_g_1 = normal_function(\n",
    "            0, 16/1.96, np.asarray([i for i in range(0, 17)])) * glucose_row[0:17]\n",
    "        normal_g_2 = normal_function(\n",
    "            16, 16/1.96, np.asarray([i for i in range(0, 33)])) * glucose_row[0:33]\n",
    "        normal_g_3 = normal_function(\n",
    "            32, 16/1.96, np.asarray([i for i in range(16, 33)])) * glucose_row[16:33]\n",
    "        for i in range(1, 4):\n",
    "            gaussian_feature.append(np.trapz(locals()[\"normal_g_\" + str(i)]))\n",
    "        return gaussian_feature\n",
    "\n",
    "\n",
    "    p,c,f,e,r=process_data(data)\n",
    "    newfeature=process_glucose(r, feature_num)\n",
    "\n",
    "    X_Gau = newfeature\n",
    "\n",
    "    #X_both = np.hstack((X_value,X_Gau))\n",
    "    \n",
    "    return X_Gau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "#training + parameter search\n",
    "######################################\n",
    "\n",
    "#return meal prediction, ground truth, and meal score\n",
    "def ConstructModel_PrintResult(gpu_num, Input_X, Y1_value_1hot, Y2_value_1hot, Y3_value_1hot, \n",
    "                               sub, sub_meal_idx, scenario = 'L1MO', N_epochs=55, verbose=True):\n",
    "\n",
    "    import os\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "    device_nb = gpu_num\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(device_nb)\n",
    "\n",
    "\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "\n",
    "    #Y1=carbs     Y2=protein      Y3=fat\n",
    "\n",
    "\n",
    "    #options: X_value     X_Gau   X_both\n",
    "    Input_X = Input_X\n",
    "\n",
    "\n",
    "\n",
    "    #loss => all default huber\n",
    "\n",
    "    #choose between L1SO vs. L1MO\n",
    "    scenario = scenario\n",
    "\n",
    "    L1MO_test_subs = [sub]\n",
    "\n",
    "    neuron_shared_ls = [5]\n",
    "\n",
    "    learning_rate_1_ls = [0.005]\n",
    "\n",
    "    learning_rate_2_ls = [0.005]\n",
    "\n",
    "    learning_rate_3_ls = [0.005]\n",
    "\n",
    "    #5 0.001 0.001 0.005\n",
    "\n",
    "\n",
    "\n",
    "    N_epochs = N_epochs\n",
    "    \n",
    "    #for reproducibility on NN\n",
    "    np.random.seed(5)\n",
    "\n",
    "    metrics_CHO = []\n",
    "    metrics_pro = []\n",
    "    metrics_fat = []\n",
    "\n",
    "\n",
    "    #to average the results for a number of runs\n",
    "    #for num_of_runs in range(30):\n",
    "\n",
    "    #____________________________________________\n",
    "    #if not for grid search, can be commented out\n",
    "    for neurons_shared in neuron_shared_ls:\n",
    "        for learning_rate_1 in learning_rate_1_ls:\n",
    "            for learning_rate_2 in learning_rate_2_ls:\n",
    "                for learning_rate_3 in learning_rate_3_ls:\n",
    "\n",
    "                    '''\n",
    "                    neurons_shared = 3\n",
    "                    learning_rate_1 = 0.001\n",
    "                    learning_rate_2 = 0.005\n",
    "                    learning_rate_3 = 0.005\n",
    "                    '''\n",
    "\n",
    "\n",
    "                    # ======================\n",
    "                    # Define the Graph\n",
    "                    # ======================\n",
    "\n",
    "                    # Define the Placeholders\n",
    "                    X = tf.placeholder(\"float\", [None, Input_X.shape[1]], name=\"X\")\n",
    "                    Y1 = tf.placeholder(\"float\", [None, Y1_value_1hot_.shape[1]], name=\"Y1\")\n",
    "                    Y2 = tf.placeholder(\"float\", [None, Y2_value_1hot_.shape[1]], name=\"Y2\")\n",
    "                    Y3 = tf.placeholder(\"float\", [None, Y3_value_1hot_.shape[1]], name=\"Y3\")\n",
    "\n",
    "                    # Initial random weights for the layers\n",
    "                    #shared layer weights\n",
    "                    initial_shared_layer_weights = np.random.rand(Input_X.shape[1],neurons_shared)\n",
    "                    #print(initial_shared_layer_weights)\n",
    "                    initial_Y1_layer_weights = np.random.rand(neurons_shared, Y1_value_1hot_.shape[1])\n",
    "                    #print(initial_Y1_layer_weights)\n",
    "                    initial_Y2_layer_weights = np.random.rand(neurons_shared, Y2_value_1hot_.shape[1])\n",
    "                    #print(initial_Y2_layer_weights)\n",
    "                    initial_Y3_layer_weights = np.random.rand(neurons_shared, Y3_value_1hot_.shape[1])\n",
    "                    #print(initial_Y3_layer_weights)\n",
    "\n",
    "                    #define the variable objects for the weights\n",
    "                    shared_layer_weights = tf.Variable(initial_shared_layer_weights, name=\"share_W\", dtype=\"float32\")\n",
    "                    Y1_layer_weights = tf.Variable(initial_Y1_layer_weights, name=\"share_Y1\", dtype=\"float32\")\n",
    "                    Y2_layer_weights = tf.Variable(initial_Y2_layer_weights, name=\"share_Y2\", dtype=\"float32\")\n",
    "                    Y3_layer_weights = tf.Variable(initial_Y3_layer_weights, name=\"share_Y3\", dtype=\"float32\")\n",
    "\n",
    "\n",
    "\n",
    "                    #Define the biases for the layers\n",
    "                    #shared layer bias\n",
    "                    initial_shared_layer_bias = np.random.rand(neurons_shared,)\n",
    "                    #print('bias')\n",
    "                    #print(initial_shared_layer_bias)\n",
    "                    initial_Y1_layer_bias = np.random.rand(Y1_value_1hot_.shape[1])\n",
    "                    #print(initial_Y1_layer_bias)\n",
    "                    initial_Y2_layer_bias = np.random.rand(Y2_value_1hot_.shape[1])\n",
    "                    #print(initial_Y2_layer_bias)\n",
    "                    initial_Y3_layer_bias = np.random.rand(Y3_value_1hot_.shape[1])\n",
    "                    #print(initial_Y3_layer_bias)\n",
    "\n",
    "                    shared_layer_bias = tf.Variable(initial_shared_layer_bias, name=\"share_B\", dtype=\"float32\")\n",
    "                    Y1_layer_bias = tf.Variable(initial_Y1_layer_bias, name=\"share_Y1_B\", dtype=\"float32\")\n",
    "                    Y2_layer_bias = tf.Variable(initial_Y2_layer_bias, name=\"share_Y2_B\", dtype=\"float32\")\n",
    "                    Y3_layer_bias = tf.Variable(initial_Y3_layer_bias, name=\"share_Y3_B\", dtype=\"float32\")\n",
    "\n",
    "\n",
    "\n",
    "                    # Construct the Layers with RELU Activations\n",
    "                    shared_layer = tf.nn.relu(tf.add(tf.matmul(X,shared_layer_weights), shared_layer_bias))\n",
    "                    #Y1_layer = tf.nn.relu(tf.add(tf.matmul(shared_layer,Y1_layer_weights), Y1_layer_bias))\n",
    "                    #Y2_layer = tf.nn.relu(tf.add(tf.matmul(shared_layer,Y2_layer_weights), Y2_layer_bias))\n",
    "                    #Y3_layer = tf.nn.relu(tf.add(tf.matmul(shared_layer,Y3_layer_weights), Y3_layer_bias))\n",
    "                    Y1_layer = tf.add(tf.matmul(shared_layer,Y1_layer_weights), Y1_layer_bias)\n",
    "                    Y2_layer = tf.add(tf.matmul(shared_layer,Y2_layer_weights), Y2_layer_bias)\n",
    "                    Y3_layer = tf.add(tf.matmul(shared_layer,Y3_layer_weights), Y3_layer_bias)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    # Calculate Loss\n",
    "                    #Y1_Loss = tf.nn.l2_loss(Y1-Y1_layer)\n",
    "                    #Y1_Loss = tf.norm(Y1-Y1_layer, ord= 1)\n",
    "                    #Y1_Loss = tf.losses.mean_squared_error(Y1, Y1_layer)\n",
    "                    #Y1_Loss = tf.losses.huber_loss(Y1, Y1_layer)\n",
    "                    #Y1_Loss = tf.keras.losses.logcosh(Y1, Y1_layer)\n",
    "                    Y1_Loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Y1_layer, labels=Y1))\n",
    "\n",
    "                    #Y2_Loss = tf.nn.l2_loss(Y2-Y2_layer)\n",
    "                    #Y2_Loss = tf.norm(Y2-Y2_layer, ord= 1)\n",
    "                    #Y2_Loss = tf.losses.mean_squared_error(Y2, Y2_layer)\n",
    "                    #Y2_Loss = tf.losses.huber_loss(Y2, Y2_layer)\n",
    "                    Y2_Loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Y2_layer, labels=Y2))\n",
    "\n",
    "                    #Y3_Loss = tf.nn.l2_loss(Y3-Y3_layer)\n",
    "                    #Y3_Loss = tf.norm(Y3-Y3_layer, ord= 1)\n",
    "                    #Y3_Loss = tf.losses.mean_squared_error(Y3, Y3_layer)\n",
    "                    #Y3_Loss = tf.losses.huber_loss(Y3, Y3_layer)\n",
    "                    Y3_Loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Y3_layer, labels=Y3))\n",
    "\n",
    "                    Joint_Loss = Y1_Loss + Y2_Loss + Y3_Loss\n",
    "\n",
    "                    # optimisers\n",
    "                    Optimiser = tf.train.AdamOptimizer().minimize(Joint_Loss)\n",
    "                    Y1_op = tf.train.AdamOptimizer(learning_rate=learning_rate_1, beta1=0.9, beta2=0.999, epsilon=1e-08).minimize(Y1_Loss)\n",
    "                    Y2_op = tf.train.AdamOptimizer(learning_rate=learning_rate_2, beta1=0.9, beta2=0.999, epsilon=1e-08).minimize(Y2_Loss)\n",
    "                    Y3_op = tf.train.AdamOptimizer(learning_rate=learning_rate_3, beta1=0.9, beta2=0.999, epsilon=1e-08).minimize(Y3_Loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    #__________________________________________________________________________________________________________\n",
    "                    # Joint Training\n",
    "                    # Calculation (Session) Code\n",
    "                    # ==========================\n",
    "\n",
    "                    # open the session\n",
    "                    session = tf.InteractiveSession()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    ################################ L1SO #####################################################################\n",
    "                    if scenario == 'L1SO':\n",
    "\n",
    "                        sub_to_correlation = {}\n",
    "\n",
    "\n",
    "                        for test_sub in ['38A', '38B', '38C', '38D', '38E', '38F', '38H']: \n",
    "\n",
    "\n",
    "                            if test_sub not in sub_to_correlation:\n",
    "                                sub_to_correlation[test_sub] = {}\n",
    "\n",
    "\n",
    "                            Y1_loss_list = []\n",
    "                            Y2_loss_list = []\n",
    "                            Y3_loss_list = []\n",
    "                            Joint_loss_list = []\n",
    "\n",
    "\n",
    "                            session.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "                            #######  training phase  ###########\n",
    "                            for i in range(1, N_epochs):\n",
    "\n",
    "\n",
    "                                train_meal_idx = []\n",
    "                                for one_sub in sub_meal_idx:\n",
    "                                    if one_sub != test_sub:\n",
    "                                        train_meal_idx += sub_meal_idx[one_sub]\n",
    "\n",
    "                                random.shuffle(train_meal_idx)\n",
    "\n",
    "\n",
    "                                #each meal will be one batch, and only update one out of three loss\n",
    "                                for meal_idx in train_meal_idx:\n",
    "                                    rand_nb = np.random.rand()\n",
    "                                    if rand_nb < 0.25:\n",
    "                                        _, Y1_loss = session.run([Y1_op, Y1_Loss],\n",
    "                                                        {\n",
    "                                                          X: Input_X[meal_idx, :].reshape(-1,Input_X.shape[1]),\n",
    "                                                          Y1: Y1_value_1hot[meal_idx, :].reshape(-1,3),\n",
    "                                                          Y2: Y2_value_1hot[meal_idx, :].reshape(-1,3),\n",
    "                                                          Y3: Y3_value_1hot[meal_idx, :].reshape(-1,3)\n",
    "                                                          })\n",
    "\n",
    "                                        Y1_loss_list += [float(str(Y1_loss))]\n",
    "\n",
    "                                    elif 0.25 <= rand_nb < 0.5:\n",
    "                                        _, Y2_loss = session.run([Y2_op, Y2_Loss],\n",
    "                                                        {\n",
    "                                                          X: Input_X[meal_idx, :].reshape(-1,Input_X.shape[1]),\n",
    "                                                          Y1: Y1_value_1hot[meal_idx, :].reshape(-1,3),\n",
    "                                                          Y2: Y2_value_1hot[meal_idx, :].reshape(-1,3),\n",
    "                                                          Y3: Y3_value_1hot[meal_idx, :].reshape(-1,3)\n",
    "                                                          })\n",
    "\n",
    "                                        Y2_loss_list += [float(str(Y2_loss))]\n",
    "\n",
    "                                    elif 0.5 <= rand_nb < 0.75:\n",
    "                                        _, Y3_loss = session.run([Y3_op, Y3_Loss],\n",
    "                                                        {\n",
    "                                                          X: Input_X[meal_idx, :].reshape(-1,Input_X.shape[1]),\n",
    "                                                          Y1: Y1_value_1hot[meal_idx, :].reshape(-1,3),\n",
    "                                                          Y2: Y2_value_1hot[meal_idx, :].reshape(-1,3),\n",
    "                                                          Y3: Y3_value_1hot[meal_idx, :].reshape(-1,3)\n",
    "                                                          })\n",
    "\n",
    "                                        Y3_loss_list += [float(str(Y3_loss))]\n",
    "\n",
    "                                    else:\n",
    "                                        _, Joint_loss = session.run([Optimiser, Joint_Loss],\n",
    "                                                        {\n",
    "                                                          X: Input_X[meal_idx, :].reshape(-1,Input_X.shape[1]),\n",
    "                                                          Y1: Y1_value_1hot[meal_idx, :].reshape(-1,3),\n",
    "                                                          Y2: Y2_value_1hot[meal_idx, :].reshape(-1,3),\n",
    "                                                          Y3: Y3_value_1hot[meal_idx, :].reshape(-1,3)\n",
    "                                                          })\n",
    "                                        Joint_loss_list += [float(str(Joint_loss))]\n",
    "\n",
    "\n",
    "\n",
    "                                if i % 2000 == 0:\n",
    "                                    print('y1 loss: ', Y1_loss, end=' |')\n",
    "                                    print('y2 loss: ', Y2_loss, end=' |')\n",
    "                                    print('y3 loss: ', Y3_loss, end=' |')\n",
    "                                    print('joint loss: ', Joint_loss)\n",
    "\n",
    "\n",
    "\n",
    "                            #######  testing phase  ###########\n",
    "                            all_pred = []\n",
    "                            all_true = []\n",
    "\n",
    "                            for test_meal in sub_meal_idx[test_sub]:\n",
    "                                Y1_pred, Y2_pred, Y3_pred = session.run([Y1_layer,Y2_layer,Y3_layer], {\n",
    "                                                      X: Input_X[test_meal].reshape(-1,Input_X.shape[1]),\n",
    "                                                      Y1: Y1_value_1hot[test_meal].reshape(-1,3),\n",
    "                                                      Y2: Y2_value_1hot[test_meal].reshape(-1,3),\n",
    "                                                      Y3: Y3_value_1hot[test_meal].reshape(-1,3)\n",
    "                                                      })\n",
    "                                all_pred += [[np.argmax(Y1_pred[0]), np.argmax(Y2_pred[0]), np.argmax(Y3_pred[0])]]\n",
    "                                all_true += [[np.argmax(Y1_value_1hot[test_meal]), np.argmax(Y2_value_1hot[test_meal]), np.argmax(Y3_value_1hot[test_meal])]]\n",
    "\n",
    "\n",
    "\n",
    "                            #######  plotting phase  ###########\n",
    "                            corr_cx = []\n",
    "                            corr_cy = []\n",
    "                            corr_px = []\n",
    "                            corr_py = []\n",
    "                            corr_fx = []\n",
    "                            corr_fy = []\n",
    "\n",
    "\n",
    "                            for meal_num in range(len(all_true)):\n",
    "                                corr_cx.append(all_pred[meal_num][0])\n",
    "                                corr_cy.append(all_true[meal_num][0])\n",
    "\n",
    "                                corr_px.append(all_pred[meal_num][1])\n",
    "                                corr_py.append(all_true[meal_num][1])\n",
    "\n",
    "                                corr_fx.append(all_pred[meal_num][2])\n",
    "                                corr_fy.append(all_true[meal_num][2])    \n",
    "\n",
    "                            sub_to_correlation[test_sub] = {'c':accuracy_score(corr_cx,corr_cy),\n",
    "                                                            'p':accuracy_score(corr_px,corr_py),\n",
    "                                                            'f':accuracy_score(corr_fx,corr_fy)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                            '''\n",
    "                            plt.subplot(411)\n",
    "                            plt.plot(Y1_loss_list)\n",
    "                            plt.subplot(412)\n",
    "                            plt.plot(Y2_loss_list)\n",
    "                            plt.subplot(413)\n",
    "                            plt.plot(Y3_loss_list)\n",
    "                            plt.subplot(414)\n",
    "                            plt.plot(Joint_loss_list)\n",
    "                            plt.show()\n",
    "                            '''\n",
    "\n",
    "                            print(test_sub[-1]+'_CHO = ',corr_cx)\n",
    "                            print('truth_'+test_sub[-1]+'_CHO = ',corr_cy)\n",
    "                            print(test_sub[-1]+'_pro = ',corr_px)\n",
    "                            print('truth_'+test_sub[-1]+'_pro = ',corr_py)\n",
    "                            print(test_sub[-1]+'_fat = ',corr_fx)\n",
    "                            print('truth_'+test_sub[-1]+'_fat = ',corr_fy)\n",
    "\n",
    "\n",
    "                        print(neurons_shared, learning_rate_1, learning_rate_2, learning_rate_3) \n",
    "                        print(sub_to_correlation)\n",
    "\n",
    "\n",
    "                        print(max(Y1_loss_list))\n",
    "                        print(max(Y2_loss_list))  \n",
    "                        print(max(Y3_loss_list))\n",
    "\n",
    "\n",
    "\n",
    "                    ################################## L1MO #####################################################################       \n",
    "                    elif scenario == 'L1MO':\n",
    "\n",
    "                        sub_to_correlation = {}\n",
    "\n",
    "                        #for test_sub in ['38A', '38B', '38C', '38D', '38E', '38F', '38H']: \n",
    "                        #for test_sub in ['38B']: \n",
    "                        for test_sub in L1MO_test_subs:\n",
    "\n",
    "                            \n",
    "                            test_sub_BW = sub_BW[test_sub]\n",
    "                            test_sub_BMI = sub_BMI[test_sub]\n",
    "                            test_sub_LBM = sub_LBM[test_sub]\n",
    "                            test_sub_FM = sub_FM[test_sub]\n",
    "                            \n",
    "                            test_sub_demo_info = [test_sub_BW, test_sub_BMI, test_sub_LBM, test_sub_FM]\n",
    "                            \n",
    "                            if test_sub not in sub_to_correlation:\n",
    "                                 sub_to_correlation[test_sub] = {}\n",
    "\n",
    "\n",
    "                            Y1_loss_list = []\n",
    "                            Y2_loss_list = []\n",
    "                            Y3_loss_list = []\n",
    "                            Joint_loss_list = []\n",
    "\n",
    "                            session.run(tf.global_variables_initializer())\n",
    "\n",
    "                            #For calculating AUC purpose\n",
    "                            all_meal_score = []\n",
    "\n",
    "                            all_meal_pred = []\n",
    "                            all_meal_true = []\n",
    "\n",
    "                            for test_meal in sub_meal_idx[test_sub]:\n",
    "\n",
    "                                #create training indexes\n",
    "                                train_meal_idxes = []\n",
    "                                for meal_index in sub_meal_idx[test_sub]:\n",
    "                                    if meal_index != test_meal:\n",
    "                                        train_meal_idxes.append(meal_index) \n",
    "\n",
    "                                print(test_sub, test_meal, train_meal_idxes)\n",
    "                                #random.shuffle(train_meal_idxes)\n",
    "\n",
    "\n",
    "                                #######  training phase  ###########\n",
    "                                for i in range(1,N_epochs):\n",
    "                                    for meal_idx in train_meal_idxes:\n",
    "\n",
    "                                        rand_nb = np.random.rand()\n",
    "                                        #if rand_nb < 0.333:\n",
    "                                        _, Y1_loss = session.run([Y1_op, Y1_Loss],\n",
    "                                                        {\n",
    "                                                          X: Input_X[train_meal_idxes, :].reshape(-1,Input_X.shape[1]),\n",
    "                                                          Y1: Y1_value_1hot[train_meal_idxes, :].reshape(-1,3),\n",
    "                                                          Y2: Y2_value_1hot[train_meal_idxes, :].reshape(-1,3),\n",
    "                                                          Y3: Y3_value_1hot[train_meal_idxes, :].reshape(-1,3)\n",
    "                                                          })\n",
    "\n",
    "                                        Y1_loss_list += [float(str(Y1_loss))]\n",
    "\n",
    "                                        #elif 0.333 <= rand_nb < 0.666:\n",
    "                                        _, Y2_loss = session.run([Y2_op, Y2_Loss],\n",
    "                                                        {\n",
    "                                                          X: Input_X[train_meal_idxes, :].reshape(-1,Input_X.shape[1]),\n",
    "                                                          Y1: Y1_value_1hot[train_meal_idxes, :].reshape(-1,3),\n",
    "                                                          Y2: Y2_value_1hot[train_meal_idxes, :].reshape(-1,3),\n",
    "                                                          Y3: Y3_value_1hot[train_meal_idxes, :].reshape(-1,3)\n",
    "                                                          })\n",
    "\n",
    "                                        Y2_loss_list += [float(str(Y2_loss))]\n",
    "\n",
    "                                        #elif 0.666 <= rand_nb < 0.999:\n",
    "                                        _, Y3_loss = session.run([Y3_op, Y3_Loss],\n",
    "                                                        {\n",
    "                                                          X: Input_X[train_meal_idxes, :].reshape(-1,Input_X.shape[1]),\n",
    "                                                          Y1: Y1_value_1hot[train_meal_idxes, :].reshape(-1,3),\n",
    "                                                          Y2: Y2_value_1hot[train_meal_idxes, :].reshape(-1,3),\n",
    "                                                          Y3: Y3_value_1hot[train_meal_idxes, :].reshape(-1,3)\n",
    "                                                          })\n",
    "\n",
    "                                        Y3_loss_list += [float(str(Y3_loss))]\n",
    "\n",
    "                                        '''    \n",
    "                                        else:\n",
    "                                            _, Joint_loss = session.run([Optimiser, Joint_Loss],\n",
    "                                                            {\n",
    "                                                              X: Input_X[meal_idx, :].reshape(-1,Input_X.shape[1]),\n",
    "                                                              Y1: Y1_value[meal_idx, :].reshape(-1,1),\n",
    "                                                              Y2: Y2_value[meal_idx, :].reshape(-1,1),\n",
    "                                                              Y3: Y3_value[meal_idx, :].reshape(-1,1)\n",
    "                                                              })\n",
    "                                            Joint_loss_list += [float(str(Joint_loss))]\n",
    "                                        '''\n",
    "\n",
    "\n",
    "                                if i % 50 == 0:\n",
    "                                    print('y1 loss: ', Y1_loss, end=' |')\n",
    "                                    print('y2 loss: ', Y2_loss, end=' |')\n",
    "                                    print('y3 loss: ', Y3_loss, end=' |')\n",
    "                                    print('joint loss: ', Joint_loss)\n",
    "\n",
    "\n",
    "\n",
    "                                #######  testing phase  ###########\n",
    "                                Y1_pred, Y2_pred, Y3_pred = session.run([Y1_layer,Y2_layer,Y3_layer], {\n",
    "                                                      X: Input_X[test_meal].reshape(-1,Input_X.shape[1]),\n",
    "                                                      Y1: Y1_value_1hot[test_meal].reshape(-1,3),\n",
    "                                                      Y2: Y2_value_1hot[test_meal].reshape(-1,3),\n",
    "                                                      Y3: Y3_value_1hot[test_meal].reshape(-1,3)\n",
    "                                                      })\n",
    "                                \n",
    "                                \n",
    "                                all_meal_score += [[Y1_pred[0], Y2_pred[0], Y3_pred[0]]]\n",
    "                                all_meal_pred += [[np.argmax(Y1_pred[0]), np.argmax(Y2_pred[0]), np.argmax(Y3_pred[0])]]\n",
    "                                all_meal_true += [[np.argmax(Y1_value_1hot[test_meal]), np.argmax(Y2_value_1hot[test_meal]), np.argmax(Y3_value_1hot[test_meal])]]\n",
    "\n",
    "\n",
    "\n",
    "                            #######  plotting phase  ###########\n",
    "                            corr_cx = []\n",
    "                            corr_cy = []\n",
    "                            corr_px = []\n",
    "                            corr_py = []\n",
    "                            corr_fx = []\n",
    "                            corr_fy = []\n",
    "\n",
    "\n",
    "                            for meal_num in range(len(all_meal_true)):\n",
    "                                corr_cx.append(all_meal_pred[meal_num][0])\n",
    "                                corr_cy.append(all_meal_true[meal_num][0])\n",
    "\n",
    "                                corr_px.append(all_meal_pred[meal_num][1])\n",
    "                                corr_py.append(all_meal_true[meal_num][1])\n",
    "\n",
    "                                corr_fx.append(all_meal_pred[meal_num][2])\n",
    "                                corr_fy.append(all_meal_true[meal_num][2])    \n",
    "\n",
    "                            sub_to_correlation[test_sub] = {'c':macro_recall(confusion_matrix(corr_cy, corr_cx)),\n",
    "                                                            'p':macro_recall(confusion_matrix(corr_py, corr_px)),\n",
    "                                                            'f':macro_recall(confusion_matrix(corr_fy, corr_fx))}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                            plt.subplot(411)\n",
    "                            plt.plot(Y1_loss_list)\n",
    "                            plt.subplot(412)\n",
    "                            plt.plot(Y2_loss_list)\n",
    "                            plt.subplot(413)\n",
    "                            plt.plot(Y3_loss_list)\n",
    "                            plt.subplot(414)\n",
    "                            plt.plot(Joint_loss_list)\n",
    "                            plt.show()\n",
    "\n",
    "\n",
    "                        print(neurons_shared, learning_rate_1, learning_rate_2, learning_rate_3)    \n",
    "                        print(sub_to_correlation)\n",
    "\n",
    "                        print('c')\n",
    "                        print(corr_cx, corr_cy)\n",
    "                        print(confusion_matrix(corr_cy, corr_cx))\n",
    "                        print('p')\n",
    "                        print(corr_px, corr_py)\n",
    "                        print(confusion_matrix(corr_py, corr_px))\n",
    "                        print('f')\n",
    "                        print(corr_fx, corr_fy) \n",
    "                        print(confusion_matrix(corr_fy, corr_fx))\n",
    "\n",
    "                        metrics_CHO.append(macro_recall(confusion_matrix(corr_cy, corr_cx)))\n",
    "                        metrics_pro.append(macro_recall(confusion_matrix(corr_py, corr_px)))\n",
    "                        metrics_fat.append(macro_recall(confusion_matrix(corr_fy, corr_fx)))\n",
    "\n",
    "\n",
    "\n",
    "                        #print(max(Y1_loss_list))\n",
    "                        #print(max(Y2_loss_list))  \n",
    "                        #print(max(Y3_loss_list))\n",
    "\n",
    "\n",
    "    print(metrics_CHO, metrics_pro, metrics_fat)\n",
    "\n",
    "    all_meal_score = np.asarray(all_meal_score)\n",
    "    all_meal_true = np.asarray(all_meal_true)\n",
    "    \n",
    "    return all_meal_score, all_meal_pred, all_meal_true, session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()\n",
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluation_results(all_meal_score, all_meal_true, component = 0):\n",
    "\n",
    "    def softmax(x):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0) # only difference\n",
    "\n",
    "\n",
    "    #change logit number to probability\n",
    "    def row_wise_softmax(all_meal_score):\n",
    "        y_score = []\n",
    "\n",
    "        for row in all_meal_score[:,0,:]:\n",
    "            y_score.append(softmax(row))\n",
    "\n",
    "        y_score = np.asarray(y_score)\n",
    "\n",
    "        return y_score\n",
    "\n",
    "\n",
    "\n",
    "    #carb: 0    protein: 1     fat:2\n",
    "    def dummify_one_component(all_meal_true, component = 0):\n",
    "\n",
    "        if all_meal_true.ndim >= 2:\n",
    "\n",
    "            y_test = label_binarize(all_meal_true[:,component], classes=np.unique(all_meal_true[:,component]))\n",
    "\n",
    "        else:\n",
    "\n",
    "            y_test\n",
    "\n",
    "        return y_test\n",
    "    \n",
    "    \n",
    "    #prepare labels and proba for one of the components\n",
    "    y_score = row_wise_softmax(all_meal_score)\n",
    "    y_test = dummify_one_component(all_meal_true, component)\n",
    "    \n",
    "    return y_score, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run all experiment in one line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-35316934ebef>:139: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "38H 54 [55, 56, 58, 59, 61, 62]\n",
      "38H 55 [54, 56, 58, 59, 61, 62]\n",
      "38H 56 [54, 55, 58, 59, 61, 62]\n",
      "38H 58 [54, 55, 56, 59, 61, 62]\n",
      "38H 59 [54, 55, 56, 58, 61, 62]\n",
      "38H 61 [54, 55, 56, 58, 59, 62]\n",
      "38H 62 [54, 55, 56, 58, 59, 61]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0.005 0.005 0.005\n",
      "{'38H': {'c': 0.6666666666666666, 'p': 0.4166666666666667, 'f': 0.7222222222222222}}\n",
      "c\n",
      "[1, 1, 2, 2, 1, 1, 1] [0, 1, 2, 2, 1, 1, 1]\n",
      "[[0 1 0]\n",
      " [0 4 0]\n",
      " [0 0 2]]\n",
      "p\n",
      "[0, 2, 1, 2, 1, 2, 1] [0, 1, 2, 1, 2, 1, 1]\n",
      "[[1 0 0]\n",
      " [0 1 3]\n",
      " [0 2 0]]\n",
      "f\n",
      "[0, 1, 1, 2, 1, 2, 0] [0, 1, 2, 1, 1, 2, 0]\n",
      "[[2 0 0]\n",
      " [0 2 1]\n",
      " [0 1 1]]\n",
      "[0.6666666666666666] [0.4166666666666667] [0.7222222222222222]\n"
     ]
    }
   ],
   "source": [
    "blood_analyte = 'CGM'\n",
    "spreadsheet = '../../CGM_insulin_TG_data.xlsx'\n",
    "\n",
    "\n",
    "#get data with different options\n",
    "X_value, Y1_value_1hot_, Y2_value_1hot_, Y3_value_1hot_, sub_meal_idx = get_raw_data(blood_analyte, spreadsheet)\n",
    "\n",
    "X_Gau = get_Gau_features(blood_analyte, spreadsheet, feature_num=5)\n",
    "\n",
    "sub = '38H'\n",
    "\n",
    "#which component to plot\n",
    "#0: carb  1: protein  2: fat\n",
    "component = 2\n",
    "\n",
    "#train/build model with different options\n",
    "all_meal_score, all_meal_pred, all_meal_true, session = ConstructModel_PrintResult(gpu_num=0, \n",
    "                                                                                   Input_X=X_Gau,  #X_Gau or X_value\n",
    "                                                                                   Y1_value_1hot = Y1_value_1hot_,\n",
    "                                                                                   Y2_value_1hot = Y2_value_1hot_,\n",
    "                                                                                   Y3_value_1hot = Y3_value_1hot_,\n",
    "                                                                                   sub=sub, \n",
    "                                                                                   sub_meal_idx=sub_meal_idx,\n",
    "                                                                                   scenario = 'L1MO',#'L1MO' or 'L1SO'\n",
    "                                                                                   N_epochs=55,\n",
    "                                                                                   verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#perdicted scores and ground truth\n",
    "#component = 0 carb, 1 protein, 2 fat\n",
    "y_score_c, y_test_c = Evaluation_results(all_meal_score, all_meal_true, component = 0)\n",
    "\n",
    "y_score_p, y_test_p = Evaluation_results(all_meal_score, all_meal_true, component = 1)\n",
    "\n",
    "y_score_f, y_test_f = Evaluation_results(all_meal_score, all_meal_true, component = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn micro AUROC\n",
      "carbs: 0.9489795918367346\n",
      "protein: 0.8265306122448979\n",
      "fat: 0.8163265306122448\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "average = 'micro'\n",
    "print('sklearn '+ average + ' AUROC')\n",
    "print('carbs:', roc_auc_score(y_test_c, y_score_c, average=average))\n",
    "print('protein:', roc_auc_score(y_test_p, y_score_p, average=average))\n",
    "print('fat:', roc_auc_score(y_test_f, y_score_f, average=average))\n",
    "\n",
    "\n",
    "#macro: Calculate metrics for each label, and find their unweighted mean. So different my method\n",
    "#micro: same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "\n",
    "\n",
    "def plot_AUROC_one_component(y_score, y_test, component):\n",
    "\n",
    "    n_classes = y_test.shape[1]\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "        #fpr\n",
    "        #array([0. , 0. , 0.5, 0.5, 1. ])\n",
    "        \n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        \n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "    #fpr,         tpr,         threshold\n",
    "    \n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr[2], tpr[2], color='cornflowerblue',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Compute macro-average ROC curve and ROC area\n",
    "\n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label='micro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"micro\"]),\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "             label='macro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"macro\"]),\n",
    "             color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                 label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                 ''.format(i, roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    if component == 0:\n",
    "        comp = 'CHO'\n",
    "    elif component == 1:\n",
    "        comp = 'protein'\n",
    "    elif component == 2:\n",
    "        comp = 'fat'\n",
    "    plt.title('AUROC to %s multi-class prediction for %s'%(comp, sub))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    return fpr, tpr, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-27d3425f7a85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot one of the classes in one component\n",
    "\n",
    "#carb: 0 , protein: 1 , fat: 2 \n",
    "component = component\n",
    "\n",
    "if component == 0:\n",
    "    y_score = y_score_c\n",
    "    y_test = y_test_c\n",
    "elif component == 1:\n",
    "    y_score = y_score_p\n",
    "    y_test = y_test_p\n",
    "elif component == 2:\n",
    "    y_score = y_score_f\n",
    "    y_test = y_test_f\n",
    "\n",
    "\n",
    "\n",
    "fpr, tpr, roc_auc = plot_AUROC_one_component(y_score, y_test, component = component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()\n",
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python py36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
