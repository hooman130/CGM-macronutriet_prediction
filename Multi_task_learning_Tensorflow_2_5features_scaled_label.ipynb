{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "device_nb = 1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(device_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "#raw data\n",
    "########################################################\n",
    "\n",
    "\n",
    "\n",
    "#subject_id = '38A'\n",
    "blood_analyte = 'CGM'\n",
    "\n",
    "xl = pd.ExcelFile('CGM_insulin_TG_data.xlsx')\n",
    "Meal_dfs = xl.parse(blood_analyte)\n",
    "sub_meal_idx = {}\n",
    "\n",
    "sub_LBM = {'38A': 39.47,\n",
    "           '38B': 62.78,\n",
    "           '38C': 53.5,\n",
    "           '38D': 37.47,\n",
    "           '38E': 45.52,\n",
    "           '38F': 43.38,\n",
    "           '38H': 47.39,}\n",
    "\n",
    "X=[]\n",
    "Y1=[]\n",
    "Y2=[]\n",
    "Y3=[]\n",
    "\n",
    "for index, row in Meal_dfs.iterrows():\n",
    "    \n",
    "    #ignore missing meals\n",
    "    if not math.isnan(row.tolist()[6]):\n",
    "\n",
    "        #get each subject's meal index\n",
    "        for subject_id in ['38A', '38B', '38C', '38D', '38E', '38F', '38H']:\n",
    "            if row['Patient_ID'] == subject_id:\n",
    "                if subject_id in sub_meal_idx:\n",
    "                    sub_meal_idx[subject_id] += [index]\n",
    "                else:\n",
    "                    sub_meal_idx[subject_id] = [index]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    X += [np.float32(one_cgm) for one_cgm in row.tolist()[2:34]]\n",
    "\n",
    "    #carbs\n",
    "    if row['Meal_ID'][4]== '1':\n",
    "        Y1 += [np.float32(52.25/sub_LBM[row[0]])]\n",
    "        #Y1 += [np.float32(1)]\n",
    "    elif row['Meal_ID'][4]== '2':\n",
    "        Y1 += [np.float32(94.75/sub_LBM[row[0]])]\n",
    "        #Y1 += [np.float32(2)]\n",
    "    elif row['Meal_ID'][4]== '3':\n",
    "        Y1 += [np.float32(179.75/sub_LBM[row[0]])]\n",
    "        #Y1 += [np.float32(3)]\n",
    "\n",
    "    #protein\n",
    "    if row['Meal_ID'][1]== '1':\n",
    "        Y2 += [np.float32(15/sub_LBM[row[0]])]\n",
    "        #Y2 += [np.float32(1)]\n",
    "    elif row['Meal_ID'][1]== '2':\n",
    "        Y2 += [np.float32(30/sub_LBM[row[0]])]\n",
    "        #Y2 += [np.float32(2)]\n",
    "    elif row['Meal_ID'][1]== '3':\n",
    "        Y2 += [np.float32(60/sub_LBM[row[0]])]\n",
    "        #Y2 += [np.float32(3)]\n",
    "\n",
    "    #fat\n",
    "    if row['Meal_ID'][7]== '1':\n",
    "        Y3 += [np.float32(13/sub_LBM[row[0]])]    \n",
    "        #Y3 += [np.float32(1)]  \n",
    "    elif row['Meal_ID'][7]== '2':\n",
    "        Y3 += [np.float32(26/sub_LBM[row[0]])]   \n",
    "        #Y3 += [np.float32(2)]  \n",
    "    elif row['Meal_ID'][7]== '3':\n",
    "        Y3 += [np.float32(52/sub_LBM[row[0]])]  \n",
    "        #Y3 += [np.float32(3)]\n",
    "\n",
    "\n",
    "\n",
    "X_value = np.array(X).reshape(63,32)\n",
    "scaler = StandardScaler()\n",
    "X_value = scaler.fit_transform(X_value)\n",
    "Y1_value = np.array(Y1).reshape(-1,1)\n",
    "Y2_value = np.array(Y2).reshape(-1,1)\n",
    "Y3_value = np.array(Y3).reshape(-1,1)\n",
    "\n",
    "#Y1 = (Y1 - Y1.min())/(Y1.max()-Y1.min())\n",
    "#Y2 = (Y2 - Y2.min())/(Y2.max()-Y2.min())\n",
    "#Y3 = (Y3 - Y3.min())/(Y3.max()-Y3.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'38A': [0, 1, 2, 3, 4, 5, 6, 8],\n",
       " '38B': [10, 11, 12, 14, 15, 16, 17],\n",
       " '38C': [18, 20, 21, 22, 23, 24, 25, 26],\n",
       " '38D': [27, 28, 29, 30, 31, 32, 33, 34, 35],\n",
       " '38E': [36, 37, 38, 39, 40, 41, 42, 43, 44],\n",
       " '38F': [45, 46, 47, 48, 49, 50, 51, 52, 53],\n",
       " '38H': [54, 55, 56, 58, 59, 61, 62]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_meal_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#A: miss 8\\n#B: miss 1, 5\\n#C: miss 2\\n#H: miss 4, 7\\n#the others: meal 1 2 3 4 5 6 7 8 9 \\n\\nX_Gau = []\\n\\n\\nsub_letters = ['A', 'B', 'C', 'D', 'E', 'F', 'H']\\n      \\nfor sub_l in sub_letters:\\n    \\n    if sub_l == 'A':\\n        miss_meal = [8]\\n    elif sub_l == 'B':\\n        miss_meal = [1, 5]\\n    elif sub_l == 'C':\\n        miss_meal = [2]\\n    elif sub_l == 'H':\\n        miss_meal = [4, 7]\\n    else:\\n        miss_meal = []\\n    \\n    with open('./Tianlong/Gauss Features 2018 1121/%s.txt'%sub_l, 'r') as f0:\\n        \\n        # each line is a meal\\n        for line_num in range(1,10):\\n            \\n            one_meal  = []\\n            \\n            if line_num in miss_meal:\\n                \\n                X_Gau.append(np.array([np.nan for nan_num in range(17)]))\\n                \\n            else:\\n                line = f0.readline()\\n\\n                for number in line.split(' '):\\n\\n                    one_meal += [float(number)]\\n\\n                one_meal = np.array(one_meal)\\n                X_Gau += [one_meal]\\n                \\nX_Gau = np.array(X_Gau)\\n\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################################\n",
    "# 17 Gaussian AUC features (deprecated)\n",
    "########################################################\n",
    "\"\"\"\n",
    "#A: miss 8\n",
    "#B: miss 1, 5\n",
    "#C: miss 2\n",
    "#H: miss 4, 7\n",
    "#the others: meal 1 2 3 4 5 6 7 8 9 \n",
    "\n",
    "X_Gau = []\n",
    "\n",
    "\n",
    "sub_letters = ['A', 'B', 'C', 'D', 'E', 'F', 'H']\n",
    "      \n",
    "for sub_l in sub_letters:\n",
    "    \n",
    "    if sub_l == 'A':\n",
    "        miss_meal = [8]\n",
    "    elif sub_l == 'B':\n",
    "        miss_meal = [1, 5]\n",
    "    elif sub_l == 'C':\n",
    "        miss_meal = [2]\n",
    "    elif sub_l == 'H':\n",
    "        miss_meal = [4, 7]\n",
    "    else:\n",
    "        miss_meal = []\n",
    "    \n",
    "    with open('./Tianlong/Gauss Features 2018 1121/%s.txt'%sub_l, 'r') as f0:\n",
    "        \n",
    "        # each line is a meal\n",
    "        for line_num in range(1,10):\n",
    "            \n",
    "            one_meal  = []\n",
    "            \n",
    "            if line_num in miss_meal:\n",
    "                \n",
    "                X_Gau.append(np.array([np.nan for nan_num in range(17)]))\n",
    "                \n",
    "            else:\n",
    "                line = f0.readline()\n",
    "\n",
    "                for number in line.split(' '):\n",
    "\n",
    "                    one_meal += [float(number)]\n",
    "\n",
    "                one_meal = np.array(one_meal)\n",
    "                X_Gau += [one_meal]\n",
    "                \n",
    "X_Gau = np.array(X_Gau)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# 17 Gaussian AUC features\n",
    "########################################################\n",
    "\n",
    "\n",
    "data=pd.DataFrame(pd.read_excel('CGM_insulin_TG_data.xlsx'))\n",
    "\n",
    "\n",
    "def process_data(data):\n",
    "    sample_am = data.shape[0]\n",
    "    protein = data.iloc[:, 1].values.reshape(-1, 1)\n",
    "    cho = data.iloc[:, 2].values.reshape(-1, 1)\n",
    "    fat = data.iloc[:, 3].values.reshape(-1, 1)\n",
    "    # pudding_A = data_38A.iloc[:,4].values.reshape(-1,1)\n",
    "    # water = data_38A.iloc[:,5].values.reshape(-1,1)\n",
    "    total_e = data.iloc[:, 6].values.reshape(-1, 1)\n",
    "    glucose = data.iloc[:, 2:35].values.reshape(sample_am, -1)\n",
    "    #glucose_last_column = glucose[:, -1]\n",
    "    #glucose_r = np.c_[glucose, glucose_last_column]\n",
    "    return protein, cho, fat, total_e, glucose\n",
    "\n",
    "def process_glucose(glucose):\n",
    "    a = np.zeros((1, 5))\n",
    "    for row in glucose:\n",
    "        # print(glucose_gaussian_feature_4(row).shape())\n",
    "        # print(glucose_gaussian_feature_8(row).shape())\n",
    "        # print(glucose_gaussian_feature_16(row).shape())\n",
    "        glucose_gaussian_feature = np.hstack([glucose_gaussian_feature_8(row)])\n",
    "        a = np.vstack([a, glucose_gaussian_feature])\n",
    "    # print(a.shape)\n",
    "    a = np.delete(a, 0, axis=0)\n",
    "    # print(a.shape)\n",
    "    return a\n",
    "\n",
    "def normal_function(mu, sigma, x):\n",
    "    normal_function_value = (1 / (sigma * np.sqrt(2 * np.pi))) * \\\n",
    "        np.exp(-((x - mu) ** 2) / (2 * (sigma ** 2)))\n",
    "    return normal_function_value\n",
    "\n",
    "def glucose_gaussian_feature_4(glucose_row):\n",
    "    gaussian_feature = []\n",
    "    normal_g_1 = normal_function(\n",
    "        0, 4/1.96, np.asarray([i for i in range(0, 5)])) * glucose_row[0:5]\n",
    "    normal_g_2 = normal_function(\n",
    "        4, 4/1.96, np.asarray([i for i in range(0, 9)])) * glucose_row[0:9]\n",
    "    normal_g_3 = normal_function(\n",
    "        8, 4/1.96, np.asarray([i for i in range(4, 13)])) * glucose_row[4:13]\n",
    "    normal_g_4 = normal_function(\n",
    "        12, 4/1.96, np.asarray([i for i in range(8, 17)])) * glucose_row[8:17]\n",
    "    normal_g_5 = normal_function(\n",
    "        16, 4/1.96, np.asarray([i for i in range(12, 21)])) * glucose_row[12:21]\n",
    "    normal_g_6 = normal_function(\n",
    "        20, 4/1.96, np.asarray([i for i in range(16, 25)])) * glucose_row[16:25]\n",
    "    normal_g_7 = normal_function(\n",
    "        24, 4/1.96, np.asarray([i for i in range(20, 29)])) * glucose_row[20:29]\n",
    "    normal_g_8 = normal_function(\n",
    "        28, 4/1.96, np.asarray([i for i in range(24, 33)])) * glucose_row[24:33]\n",
    "    normal_g_9 = normal_function(\n",
    "        32, 4/1.96, np.asarray([i for i in range(28, 33)])) * glucose_row[28:33]\n",
    "    for i in range(1, 10):\n",
    "        gaussian_feature.append(np.trapz(locals()[\"normal_g_\" + str(i)]))\n",
    "    return gaussian_feature\n",
    "\n",
    "def glucose_gaussian_feature_8(glucose_row):\n",
    "    gaussian_feature = []\n",
    "    normal_g_1 = normal_function(\n",
    "        0, 8/1.96, np.asarray([i for i in range(0, 9)])) * glucose_row[0:9]\n",
    "    normal_g_2 = normal_function(\n",
    "        8, 8/1.96, np.asarray([i for i in range(0, 17)])) * glucose_row[0:17]\n",
    "    normal_g_3 = normal_function(\n",
    "        16, 8/1.96, np.asarray([i for i in range(8, 25)])) * glucose_row[8:25]\n",
    "    normal_g_4 = normal_function(\n",
    "        24, 8/1.96, np.asarray([i for i in range(16, 33)])) * glucose_row[16:33]\n",
    "    normal_g_5 = normal_function(\n",
    "        32, 8/1.96, np.asarray([i for i in range(24, 33)])) * glucose_row[24:33]\n",
    "    for i in range(1, 6):\n",
    "        gaussian_feature.append(np.trapz(locals()[\"normal_g_\" + str(i)]))\n",
    "    return gaussian_feature\n",
    "\n",
    "def glucose_gaussian_feature_16(glucose_row):\n",
    "    gaussian_feature = []\n",
    "    normal_g_1 = normal_function(\n",
    "        0, 16/1.96, np.asarray([i for i in range(0, 17)])) * glucose_row[0:17]\n",
    "    normal_g_2 = normal_function(\n",
    "        16, 16/1.96, np.asarray([i for i in range(0, 33)])) * glucose_row[0:33]\n",
    "    normal_g_3 = normal_function(\n",
    "        32, 16/1.96, np.asarray([i for i in range(16, 33)])) * glucose_row[16:33]\n",
    "    for i in range(1, 4):\n",
    "        gaussian_feature.append(np.trapz(locals()[\"normal_g_\" + str(i)]))\n",
    "    return gaussian_feature\n",
    "\n",
    "\n",
    "p,c,f,e,r=process_data(data)\n",
    "newfeature=process_glucose(r)\n",
    "\n",
    "X_Gau = newfeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014318031361695611"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_function(0,8/1.96,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/g/guangzhou92/enter/envs/py36/lib/python3.7/site-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "[1.8088055, 1.9926975, 1.3764513, 1.3764513, 3.0154686, 1.3764513, 1.3853207, 1.6126454]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-4ca72a7fce17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    291\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sub\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorr_cx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'truth_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtest_sub\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_CHO = '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcorr_cx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sub\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_CHO = '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcorr_cy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'truth_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtest_sub\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_pro = '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcorr_px\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "#training + parameter search\n",
    "######################################\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import numpy as np\n",
    "\n",
    "#Y1=carbs     Y2=protein      Y3=fat\n",
    "\n",
    "\n",
    "#options: X_value     X_Gau\n",
    "Input_X = X_Gau\n",
    "\n",
    "#loss => all default huber\n",
    "\n",
    "#choose between L1SO vs. L1MO\n",
    "scenario = 'L1SO'\n",
    "\n",
    "L1MO_test_subs = ['38A', '38B', '38C', '38D', '38E', '38F', '38H']\n",
    "\n",
    "neuron_shared_ls = [2, 3, 4, 5]\n",
    "\n",
    "learning_rate_1_ls = [0.0005, 0.001, 0.005]\n",
    "\n",
    "learning_rate_2_ls = [0.001, 0.005]\n",
    "\n",
    "learning_rate_3_ls = [0.001, 0.005]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "N_epochs = 111\n",
    "\n",
    "\n",
    "#____________________________________________\n",
    "#if not for grid search, can be commented out\n",
    "\n",
    "for neurons_shared in neuron_shared_ls:\n",
    "    for learning_rate_1 in learning_rate_1_ls:\n",
    "        for learning_rate_2 in learning_rate_2_ls:\n",
    "            for learning_rate_3 in learning_rate_3_ls:\n",
    "\n",
    "                '''\n",
    "                neurons_shared = 3\n",
    "                learning_rate_1 = 0.001\n",
    "                learning_rate_2 = 0.005\n",
    "                learning_rate_3 = 0.005\n",
    "                '''\n",
    "\n",
    "\n",
    "                # ======================\n",
    "                # Define the Graph\n",
    "                # ======================\n",
    "\n",
    "                # Define the Placeholders\n",
    "                X = tf.placeholder(\"float\", [None, Input_X.shape[1]], name=\"X\")\n",
    "                Y1 = tf.placeholder(\"float\", [None, 1], name=\"Y1\")\n",
    "                Y2 = tf.placeholder(\"float\", [None, 1], name=\"Y2\")\n",
    "                Y3 = tf.placeholder(\"float\", [None, 1], name=\"Y3\")\n",
    "\n",
    "                # Define the weights for the layers\n",
    "                initial_shared_layer_weights = np.random.rand(Input_X.shape[1],neurons_shared)\n",
    "                initial_Y1_layer_weights = np.random.rand(neurons_shared,1)\n",
    "                initial_Y2_layer_weights = np.random.rand(neurons_shared,1)\n",
    "                initial_Y3_layer_weights = np.random.rand(neurons_shared,1)\n",
    "\n",
    "                shared_layer_weights = tf.Variable(initial_shared_layer_weights, name=\"share_W\", dtype=\"float32\")\n",
    "                Y1_layer_weights = tf.Variable(initial_Y1_layer_weights, name=\"share_Y1\", dtype=\"float32\")\n",
    "                Y2_layer_weights = tf.Variable(initial_Y2_layer_weights, name=\"share_Y2\", dtype=\"float32\")\n",
    "                Y3_layer_weights = tf.Variable(initial_Y3_layer_weights, name=\"share_Y3\", dtype=\"float32\")\n",
    "\n",
    "\n",
    "\n",
    "                #Define the biases for the layers\n",
    "                initial_shared_layer_bias = np.random.rand(neurons_shared,)\n",
    "                initial_Y1_layer_bias = np.random.rand(1)\n",
    "                initial_Y2_layer_bias = np.random.rand(1)\n",
    "                initial_Y3_layer_bias = np.random.rand(1)\n",
    "\n",
    "                shared_layer_bias = tf.Variable(initial_shared_layer_bias, name=\"share_B\", dtype=\"float32\")\n",
    "                Y1_layer_bias = tf.Variable(initial_Y1_layer_bias, name=\"share_Y1_B\", dtype=\"float32\")\n",
    "                Y2_layer_bias = tf.Variable(initial_Y2_layer_bias, name=\"share_Y2_B\", dtype=\"float32\")\n",
    "                Y3_layer_bias = tf.Variable(initial_Y3_layer_bias, name=\"share_Y3_B\", dtype=\"float32\")\n",
    "\n",
    "\n",
    "\n",
    "                # Construct the Layers with RELU Activations\n",
    "                shared_layer = tf.nn.relu(tf.add(tf.matmul(X,shared_layer_weights), shared_layer_bias))\n",
    "                #Y1_layer = tf.nn.relu(tf.add(tf.matmul(shared_layer,Y1_layer_weights), Y1_layer_bias))\n",
    "                #Y2_layer = tf.nn.relu(tf.add(tf.matmul(shared_layer,Y2_layer_weights), Y2_layer_bias))\n",
    "                #Y3_layer = tf.nn.relu(tf.add(tf.matmul(shared_layer,Y3_layer_weights), Y3_layer_bias))\n",
    "                Y1_layer = tf.add(tf.matmul(shared_layer,Y1_layer_weights), Y1_layer_bias)\n",
    "                Y2_layer = tf.add(tf.matmul(shared_layer,Y2_layer_weights), Y2_layer_bias)\n",
    "                Y3_layer = tf.add(tf.matmul(shared_layer,Y3_layer_weights), Y3_layer_bias)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # Calculate Loss\n",
    "                #Y1_Loss = tf.nn.l2_loss(Y1-Y1_layer)\n",
    "                #Y1_Loss = tf.norm(Y1-Y1_layer, ord= 1)\n",
    "                #Y1_Loss = tf.losses.mean_squared_error(Y1, Y1_layer)\n",
    "                Y1_Loss = tf.losses.huber_loss(Y1, Y1_layer)\n",
    "                #Y1_Loss = tf.keras.losses.logcosh(Y1, Y1_layer)\n",
    "\n",
    "                #Y2_Loss = tf.nn.l2_loss(Y2-Y2_layer)\n",
    "                #Y2_Loss = tf.norm(Y2-Y2_layer, ord= 1)\n",
    "                #Y2_Loss = tf.losses.mean_squared_error(Y2, Y2_layer)\n",
    "                Y2_Loss = tf.losses.huber_loss(Y2, Y2_layer)\n",
    "\n",
    "                #Y3_Loss = tf.nn.l2_loss(Y3-Y3_layer)\n",
    "                #Y3_Loss = tf.norm(Y3-Y3_layer, ord= 1)\n",
    "                #Y3_Loss = tf.losses.mean_squared_error(Y3, Y3_layer)\n",
    "                Y3_Loss = tf.losses.huber_loss(Y3, Y3_layer)\n",
    "\n",
    "                Joint_Loss = Y1_Loss + Y2_Loss + Y3_Loss\n",
    "\n",
    "                # optimisers\n",
    "                Optimiser = tf.train.AdamOptimizer().minimize(Joint_Loss)\n",
    "                Y1_op = tf.train.AdamOptimizer(learning_rate=learning_rate_1, beta1=0.9, beta2=0.999, epsilon=1e-08).minimize(Y1_Loss)\n",
    "                Y2_op = tf.train.AdamOptimizer(learning_rate=learning_rate_2, beta1=0.9, beta2=0.999, epsilon=1e-08).minimize(Y2_Loss)\n",
    "                Y3_op = tf.train.AdamOptimizer(learning_rate=learning_rate_3, beta1=0.9, beta2=0.999, epsilon=1e-08).minimize(Y3_Loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #__________________________________________________________________________________________________________\n",
    "                # Joint Training\n",
    "                # Calculation (Session) Code\n",
    "                # ==========================\n",
    "\n",
    "                # open the session\n",
    "                session = tf.InteractiveSession()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                ############### L1SO ####################\n",
    "                if scenario == 'L1SO':\n",
    "\n",
    "                    sub_to_correlation = {}\n",
    "\n",
    "\n",
    "                    for test_sub in ['38A', '38B', '38C', '38D', '38E', '38F', '38H']: \n",
    "\n",
    "\n",
    "                        if test_sub not in sub_to_correlation:\n",
    "                            sub_to_correlation[test_sub] = {}\n",
    "\n",
    "\n",
    "                        Y1_loss_list = []\n",
    "                        Y2_loss_list = []\n",
    "                        Y3_loss_list = []\n",
    "                        Joint_loss_list = []\n",
    "\n",
    "\n",
    "                        session.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "                        #######  training phase  ###########\n",
    "                        for i in range(1, N_epochs):\n",
    "\n",
    "\n",
    "                            train_meal_idx = []\n",
    "                            for one_sub in sub_meal_idx:\n",
    "                                if one_sub != test_sub:\n",
    "                                    train_meal_idx += sub_meal_idx[one_sub]\n",
    "\n",
    "                            random.shuffle(train_meal_idx)\n",
    "\n",
    "\n",
    "                            #each meal will be one batch, and only update one out of three loss\n",
    "                            for meal_idx in train_meal_idx:\n",
    "                                rand_nb = np.random.rand()\n",
    "                                if rand_nb < 0.25:\n",
    "                                    _, Y1_loss = session.run([Y1_op, Y1_Loss],\n",
    "                                                    {\n",
    "                                                      X: Input_X[meal_idx, :].reshape(-1,Input_X.shape[1]),\n",
    "                                                      Y1: Y1_value[meal_idx, :].reshape(-1,1),\n",
    "                                                      Y2: Y2_value[meal_idx, :].reshape(-1,1),\n",
    "                                                      Y3: Y3_value[meal_idx, :].reshape(-1,1)\n",
    "                                                      })\n",
    "\n",
    "                                    Y1_loss_list += [float(str(Y1_loss))]\n",
    "\n",
    "                                elif 0.25 <= rand_nb < 0.5:\n",
    "                                    _, Y2_loss = session.run([Y2_op, Y2_Loss],\n",
    "                                                    {\n",
    "                                                      X: Input_X[meal_idx, :].reshape(-1,Input_X.shape[1]),\n",
    "                                                      Y1: Y1_value[meal_idx, :].reshape(-1,1),\n",
    "                                                      Y2: Y2_value[meal_idx, :].reshape(-1,1),\n",
    "                                                      Y3: Y3_value[meal_idx, :].reshape(-1,1)\n",
    "                                                      })\n",
    "\n",
    "                                    Y2_loss_list += [float(str(Y2_loss))]\n",
    "\n",
    "                                elif 0.5 <= rand_nb < 0.75:\n",
    "                                    _, Y3_loss = session.run([Y3_op, Y3_Loss],\n",
    "                                                    {\n",
    "                                                      X: Input_X[meal_idx, :].reshape(-1,Input_X.shape[1]),\n",
    "                                                      Y1: Y1_value[meal_idx, :].reshape(-1,1),\n",
    "                                                      Y2: Y2_value[meal_idx, :].reshape(-1,1),\n",
    "                                                      Y3: Y3_value[meal_idx, :].reshape(-1,1)\n",
    "                                                      })\n",
    "\n",
    "                                    Y3_loss_list += [float(str(Y3_loss))]\n",
    "\n",
    "                                else:\n",
    "                                    _, Joint_loss = session.run([Optimiser, Joint_Loss],\n",
    "                                                    {\n",
    "                                                      X: Input_X[meal_idx, :].reshape(-1,Input_X.shape[1]),\n",
    "                                                      Y1: Y1_value[meal_idx, :].reshape(-1,1),\n",
    "                                                      Y2: Y2_value[meal_idx, :].reshape(-1,1),\n",
    "                                                      Y3: Y3_value[meal_idx, :].reshape(-1,1)\n",
    "                                                      })\n",
    "                                    Joint_loss_list += [float(str(Joint_loss))]\n",
    "\n",
    "\n",
    "\n",
    "                            if i % 2000 == 0:\n",
    "                                print('y1 loss: ', Y1_loss, end=' |')\n",
    "                                print('y2 loss: ', Y2_loss, end=' |')\n",
    "                                print('y3 loss: ', Y3_loss, end=' |')\n",
    "                                print('joint loss: ', Joint_loss)\n",
    "\n",
    "\n",
    "\n",
    "                        #######  testing phase  ###########\n",
    "                        all_pred = []\n",
    "                        all_true = []\n",
    "\n",
    "                        for test_meal in sub_meal_idx[test_sub]:\n",
    "                            Y1_pred, Y2_pred, Y3_pred = session.run([Y1_layer,Y2_layer,Y3_layer], {\n",
    "                                                  X: Input_X[test_meal].reshape(-1,Input_X.shape[1]),\n",
    "                                                  Y1: Y1_value[test_meal].reshape(-1,1),\n",
    "                                                  Y2: Y2_value[test_meal].reshape(-1,1),\n",
    "                                                  Y3: Y3_value[test_meal].reshape(-1,1)\n",
    "                                                  })\n",
    "                            all_pred += [[Y1_pred[0][0], Y2_pred[0][0], Y3_pred[0][0]]]\n",
    "                            all_true += [[Y1_value[test_meal][0], Y2_value[test_meal][0], Y3_value[test_meal][0]]]\n",
    "\n",
    "\n",
    "\n",
    "                        #######  plotting phase  ###########\n",
    "                        corr_cx = []\n",
    "                        corr_cy = []\n",
    "                        corr_px = []\n",
    "                        corr_py = []\n",
    "                        corr_fx = []\n",
    "                        corr_fy = []\n",
    "\n",
    "\n",
    "                        for meal_num in range(len(all_true)):\n",
    "                            corr_cx.append(all_pred[meal_num][0])\n",
    "                            corr_cy.append(all_true[meal_num][0])\n",
    "\n",
    "                            corr_px.append(all_pred[meal_num][1])\n",
    "                            corr_py.append(all_true[meal_num][1])\n",
    "\n",
    "                            corr_fx.append(all_pred[meal_num][2])\n",
    "                            corr_fy.append(all_true[meal_num][2])    \n",
    "\n",
    "                        sub_to_correlation[test_sub] = {'c':scipy.stats.pearsonr(corr_cx,corr_cy),\n",
    "                                                        'p':scipy.stats.pearsonr(corr_px,corr_py),\n",
    "                                                        'f':scipy.stats.pearsonr(corr_fx,corr_fy)}\n",
    "\n",
    "                        \n",
    "                       \n",
    "                        \n",
    "                        '''\n",
    "                        plt.subplot(411)\n",
    "                        plt.plot(Y1_loss_list)\n",
    "                        plt.subplot(412)\n",
    "                        plt.plot(Y2_loss_list)\n",
    "                        plt.subplot(413)\n",
    "                        plt.plot(Y3_loss_list)\n",
    "                        plt.subplot(414)\n",
    "                        plt.plot(Joint_loss_list)\n",
    "                        plt.show()\n",
    "                        '''\n",
    "                        print(test_sub[-1])\n",
    "                        print(corr_cx)\n",
    "                        print('truth_'+test_sub[-1]+'_CHO = '+corr_cx)\n",
    "                        print(test_sub[-1]+'_CHO = '+corr_cy)\n",
    "                        print('truth_'+test_sub[-1]+'_pro = '+corr_px)\n",
    "                        print(test_sub[-1]+'_pro = '+corr_py)\n",
    "                        print('truth_'+test_sub[-1]+'_fat = '+corr_fx)\n",
    "                        print(test_sub[-1]+'_fat = '+corr_fy)\n",
    "                        \n",
    "                        \n",
    "                    print(neurons_shared, learning_rate_1, learning_rate_2, learning_rate_3) \n",
    "                    print(sub_to_correlation)\n",
    "                        \n",
    "\n",
    "                    print(max(Y1_loss_list))\n",
    "                    print(max(Y2_loss_list))  \n",
    "                    print(max(Y3_loss_list))\n",
    "\n",
    "\n",
    "\n",
    "                ################# L1MO ####################       \n",
    "                elif scenario == 'L1MO':\n",
    "\n",
    "                    sub_to_correlation = {}\n",
    "\n",
    "                    #for test_sub in ['38A', '38B', '38C', '38D', '38E', '38F', '38H']: \n",
    "                    #for test_sub in ['38B']: \n",
    "                    for test_sub in L1MO_test_subs:\n",
    "\n",
    "                        if test_sub not in sub_to_correlation:\n",
    "                             sub_to_correlation[test_sub] = {}\n",
    "\n",
    "\n",
    "                        Y1_loss_list = []\n",
    "                        Y2_loss_list = []\n",
    "                        Y3_loss_list = []\n",
    "                        Joint_loss_list = []\n",
    "\n",
    "                        session.run(tf.global_variables_initializer())\n",
    "\n",
    "                        all_meal_pred = []\n",
    "                        all_meal_true = []\n",
    "\n",
    "                        for test_meal in sub_meal_idx[test_sub]:\n",
    "\n",
    "                            #create training indexes\n",
    "                            train_meal_idxes = []\n",
    "                            for meal_index in sub_meal_idx[test_sub]:\n",
    "                                if meal_index != test_meal:\n",
    "                                    train_meal_idxes.append(meal_index) \n",
    "\n",
    "                            print(test_sub, test_meal, train_meal_idxes)\n",
    "                            random.shuffle(train_meal_idxes)\n",
    "\n",
    "\n",
    "                            #######  training phase  ###########\n",
    "                            for i in range(1,N_epochs):\n",
    "                                for meal_idx in train_meal_idxes:\n",
    "\n",
    "                                    rand_nb = np.random.rand()\n",
    "                                    if rand_nb < 0.333:\n",
    "                                        _, Y1_loss = session.run([Y1_op, Y1_Loss],\n",
    "                                                        {\n",
    "                                                          X: Input_X[meal_idx, :].reshape(-1,Input_X.shape[1]),\n",
    "                                                          Y1: Y1_value[meal_idx, :].reshape(-1,1),\n",
    "                                                          Y2: Y2_value[meal_idx, :].reshape(-1,1),\n",
    "                                                          Y3: Y3_value[meal_idx, :].reshape(-1,1)\n",
    "                                                          })\n",
    "\n",
    "                                        Y1_loss_list += [float(str(Y1_loss))]\n",
    "\n",
    "                                    elif 0.333 <= rand_nb < 0.666:\n",
    "                                        _, Y2_loss = session.run([Y2_op, Y2_Loss],\n",
    "                                                        {\n",
    "                                                          X: Input_X[meal_idx, :].reshape(-1,Input_X.shape[1]),\n",
    "                                                          Y1: Y1_value[meal_idx, :].reshape(-1,1),\n",
    "                                                          Y2: Y2_value[meal_idx, :].reshape(-1,1),\n",
    "                                                          Y3: Y3_value[meal_idx, :].reshape(-1,1)\n",
    "                                                          })\n",
    "\n",
    "                                        Y2_loss_list += [float(str(Y2_loss))]\n",
    "\n",
    "                                    elif 0.666 <= rand_nb < 0.999:\n",
    "                                        _, Y3_loss = session.run([Y3_op, Y3_Loss],\n",
    "                                                        {\n",
    "                                                          X: Input_X[meal_idx, :].reshape(-1,Input_X.shape[1]),\n",
    "                                                          Y1: Y1_value[meal_idx, :].reshape(-1,1),\n",
    "                                                          Y2: Y2_value[meal_idx, :].reshape(-1,1),\n",
    "                                                          Y3: Y3_value[meal_idx, :].reshape(-1,1)\n",
    "                                                          })\n",
    "\n",
    "                                        Y3_loss_list += [float(str(Y3_loss))]\n",
    "\n",
    "                                    '''    \n",
    "                                    else:\n",
    "                                        _, Joint_loss = session.run([Optimiser, Joint_Loss],\n",
    "                                                        {\n",
    "                                                          X: Input_X[meal_idx, :].reshape(-1,Input_X.shape[1]),\n",
    "                                                          Y1: Y1_value[meal_idx, :].reshape(-1,1),\n",
    "                                                          Y2: Y2_value[meal_idx, :].reshape(-1,1),\n",
    "                                                          Y3: Y3_value[meal_idx, :].reshape(-1,1)\n",
    "                                                          })\n",
    "                                        Joint_loss_list += [float(str(Joint_loss))]\n",
    "                                    '''\n",
    "\n",
    "\n",
    "                            if i % 50 == 0:\n",
    "                                print('y1 loss: ', Y1_loss, end=' |')\n",
    "                                print('y2 loss: ', Y2_loss, end=' |')\n",
    "                                print('y3 loss: ', Y3_loss, end=' |')\n",
    "                                print('joint loss: ', Joint_loss)\n",
    "\n",
    "\n",
    "\n",
    "                            #######  testing phase  ###########\n",
    "                            Y1_pred, Y2_pred, Y3_pred = session.run([Y1_layer,Y2_layer,Y3_layer], {\n",
    "                                                  X: Input_X[test_meal].reshape(-1,Input_X.shape[1]),\n",
    "                                                  Y1: Y1_value[test_meal].reshape(-1,1),\n",
    "                                                  Y2: Y2_value[test_meal].reshape(-1,1),\n",
    "                                                  Y3: Y3_value[test_meal].reshape(-1,1)\n",
    "                                                  })\n",
    "\n",
    "                            all_meal_pred += [[Y1_pred[0][0], Y2_pred[0][0], Y3_pred[0][0]]]\n",
    "                            all_meal_true += [[Y1_value[test_meal][0], Y2_value[test_meal][0], Y3_value[test_meal][0]]]\n",
    "\n",
    "\n",
    "                        #######  plotting phase  ###########\n",
    "                        corr_cx = []\n",
    "                        corr_cy = []\n",
    "                        corr_px = []\n",
    "                        corr_py = []\n",
    "                        corr_fx = []\n",
    "                        corr_fy = []\n",
    "\n",
    "\n",
    "                        for meal_num in range(len(all_meal_true)):\n",
    "                            corr_cx.append(all_meal_pred[meal_num][0])\n",
    "                            corr_cy.append(all_meal_true[meal_num][0])\n",
    "\n",
    "                            corr_px.append(all_meal_pred[meal_num][1])\n",
    "                            corr_py.append(all_meal_true[meal_num][1])\n",
    "\n",
    "                            corr_fx.append(all_meal_pred[meal_num][2])\n",
    "                            corr_fy.append(all_meal_true[meal_num][2])    \n",
    "\n",
    "                        sub_to_correlation[test_sub] = {'c':scipy.stats.pearsonr(corr_cx,corr_cy),\n",
    "                                                        'p':scipy.stats.pearsonr(corr_px,corr_py),\n",
    "                                                        'f':scipy.stats.pearsonr(corr_fx,corr_fy)}\n",
    "\n",
    "                        \n",
    "                        \n",
    "                        '''\n",
    "                        plt.subplot(411)\n",
    "                        plt.plot(Y1_loss_list)\n",
    "                        plt.subplot(412)\n",
    "                        plt.plot(Y2_loss_list)\n",
    "                        plt.subplot(413)\n",
    "                        plt.plot(Y3_loss_list)\n",
    "                        plt.subplot(414)\n",
    "                        plt.plot(Joint_loss_list)\n",
    "                        plt.show()\n",
    "                        '''\n",
    "                        \n",
    "                    print(neurons_shared, learning_rate_1, learning_rate_2, learning_rate_3)    \n",
    "                    print(sub_to_correlation)\n",
    "                    \n",
    "                    \n",
    "                   \n",
    "\n",
    "\n",
    "                    #print(max(Y1_loss_list))\n",
    "                    #print(max(Y2_loss_list))  \n",
    "                    #print(max(Y3_loss_list))\n",
    "\n",
    "\n",
    "session.close()\n",
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_to_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sub_to_correlation['38A']['c'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meal_idxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_to_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(session.run(Joint_Loss, {\n",
    "                          X: Input_X[sub_meal_idx[test_sub][2]].reshape(-1,Input_X.shape[1]),\n",
    "                          Y1: Y1_value[sub_meal_idx[test_sub][2]].reshape(-1,1),\n",
    "                          Y2: Y2_value[sub_meal_idx[test_sub][2]].reshape(-1,1),\n",
    "                          Y3: Y3_value[sub_meal_idx[test_sub][2]].reshape(-1,1)\n",
    "                          }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred = []\n",
    "all_true = []\n",
    "\n",
    "\n",
    "for test_meal in sub_meal_idx[test_sub]:\n",
    "    Y1_pred, Y2_pred, Y3_pred = session.run([Y1_layer,Y2_layer,Y3_layer], {\n",
    "                          X: Input_X[test_meal].reshape(-1,Input_X.shape[1]),\n",
    "                          Y1: Y1_value[test_meal].reshape(-1,1),\n",
    "                          Y2: Y2_value[test_meal].reshape(-1,1),\n",
    "                          Y3: Y3_value[test_meal].reshape(-1,1)\n",
    "                          })\n",
    "    all_pred += [[Y1_pred[0][0], Y2_pred[0][0], Y3_pred[0][0]]]\n",
    "    all_true += [[Y1_value[test_meal][0], Y2_value[test_meal][0], Y3_value[test_meal][0]]]\n",
    "    print(Y1_pred, Y2_pred, Y3_pred)\n",
    "    print(Y1_value[test_meal], Y2_value[test_meal], Y3_value[test_meal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_cx = []\n",
    "corr_cy = []\n",
    "corr_px = []\n",
    "corr_py = []\n",
    "corr_fx = []\n",
    "corr_fy = []\n",
    "\n",
    "\n",
    "for meal_num in range(len(all_true)):\n",
    "    corr_cx.append(all_pred[meal_num][0])\n",
    "    corr_cy.append(all_true[meal_num][0])\n",
    "    \n",
    "    corr_px.append(all_pred[meal_num][1])\n",
    "    corr_py.append(all_true[meal_num][1])\n",
    "    \n",
    "    corr_fx.append(all_pred[meal_num][2])\n",
    "    corr_fy.append(all_true[meal_num][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.pearsonr(corr_cx,corr_cy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.pearsonr(corr_px,corr_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.pearsonr(corr_fx,corr_fy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
